# Ingress in Kubernetes

## Introduction to Ingress in Kubernetes
Let's say you are deploying an application on Kubernets for a company that has an online store selling products. Your application would be available at, for example, my-onlinestore.com. You build the application into a Docker image and deploy it on the Kubernetes cluster as a pod in a deployment. Your application needs a database, so you deploy a MySQL database as a pod and create a service of type ClusterIP called mysql-service to make it accessible to your application. Your application is now working.

## Exposing Applications to the Outside World
To make the application accessible to the outside world, you create another service, this time of type NodePort, and make your application available on a high port on the nodes in the cluster. In this example, port 38080 is allocated for the service. Users can now access your application using the URL, `http://IP-of-any-of-your-nodes:38080`. The setup works and users are able to access the application. Whenever traffic increases, you increase the number of replicas of the popd to handle the additional traffic, and the service takes care of splitting traffic between the pods.

## Cloud LoadBalancer and Service Types
If your application is hosted on-premises in your data center, this is the setup. Let us take a step back and see what you could do if you were on a public cloud environment like Google Cloud Platform. In that case, instead of creating a service of type NodePort for your application, you could set it to type LoadBalancer. When you do that, Kubernetes would still do everything that it has to do for a NodePort, which is to provision a high port for the service. But in addition to that, Kubernetes also sends a request to Google Cloud Platform to provision a network LoadBalancer for the service. On receiving the request, GCP would then automatically deploy a LoadBalancer configured to route traffic to the service ports on all nodes and return its information to Kubernetes. The LoadBalancer has an external IP that can be provided to users to access the application. In this case, you set the DNS to point to this IP and users access the application using the URL, my-online-store.com.

## Scaling Services and the Need for Ingress
Your company's business grows and you now have new services for your customers. For example, a video streaming service. Now, you want your users to be able to access your new video streaming service by going to my-online-store.com/watch. You would like to make your old application accessible at my-online-store.com/wear. Your developers develop the new video streaming application as a completely different application, as it has nothing to do with the existing one. However, to share the cluster's resources, you deploy the new application as a separate deployment within the same cluster. You create a service called video-service of type LoadBalancer. Kubernetes provisions port 38282 for this service and also provisions a network LoadBalancer on the cloud. The new LoadBalancer has a new IP. Remember, you must pay for each of these LoadBalancers and having many such LoadBalancers can inversely affect your cloud bill.


## Complexities in Traffic Routing and SSL
How do you direct traffic between each of these LoadBalancers based on the URL that the user types in? You need yet another proxy or LoadBalancer that can redirect traffic based on URLs to the different services. Every time you introduce a new service, you have to reconfigure the LoadBalancer. And finally, you also need to enable SSL for your applications so your users can access your application using HTTPS. Where do you configure that? It can be done at different levels, either at the application level itself or at the LoadBalancer level, or at the proxy server level, but which one? You do not want your developers to implement it in their applications as they would do it in different ways, and it is an additional burden for them to develop additional code to handle that. You want it to be configured in one place with minimal maintenance.

## The Role of Ingress
That is a lot of different configuration, and all of these become difficult to manage when your application scales. It requires involving different individuals in different teams. You need to configure your firewall rules for each new service, and it is expensive as well, as for each service, a new cloud native LoadBalancer needs to be provisioned. Would it not be nice if you could manage all of that within the Kubernetes cluster and have all that configuration as just another Kubernetes definition file that lives along with the rest of your application deployment files? That is where ingress comes in. Ingress helps your users access your application using a single externally accessible URL that you can configure to route traffic to different services within your cluster based on the URL path, at the same time, implement SSL security as well.

## Ingress as a Layer 7 LoadBalancer
Think of ingress as a Layer 7 LoadBalancer built into the Kubernetes cluster that can be configured using native Kubernetes primitives just like any other object that we have been working with in Kubernetes. Now, remember, even with ingress, you still need to expose it to make it accessible outside the cluster. So, you still have to either publish it as a NodePort or with a cloud native LoadBalancer. But that is just a one-time configuration. Going forward, you are going to perform all your load balancing of SSL and URL-based routing configurations on the ingress controller.

## Ingress Controllers and Resources
Without ingress, how would you do all of this? You would use a reverse proxy or a load balancing solution like Nginx or HAProxy, or Traefik. You would deploy them on your Kubernetes cluster and configure them to route traffic to other services. The configuration involves defining URL routes, configuring SSL certificates, etc. Ingress is implemented by Kubernetes in kind of the same way. You first deploy a supported solution, which happens to be any of these listed here, and then specify a set of rules to configure ingress. The solution you deploy is called an ingress controller. The set of rules you configure are called ingress resources. Ingress resources are created using definition files like the ones we have been using to create pods, deployments, and services earlier in this course.

## Deploying an Ingress Controller
A Kubernetes cluster does not come with an ingress controller by default. If you set up a cluster following the demos in this course, you will not have an ingress controller built into it. So, if you simply create ingress resources and expect them to work, they will not. Let us look at each of these in a bit more detail. As mentioned, you do not have an ingress controller on Kubernetes by default, so you must deploy one. There are a number of solutions available for ingress, a few of them being GCE, which is Google's Layer 7 HTTP load balancer, Nginx, Contour, HAProxy, Traefik, and Istio. Out of these, GCE and Nginx are currently being supported and maintained by the Kubernetes project. In this lecture, we will use Nginx as an example.

## Nginx Ingress Controller Deployment Details
These ingress controllers are not just another LoadBalancer or Nginx server. The LoadBalancer components are just a part of it. Ingress controllers have additional intelligence built into them to monitor the Kubernetes cluster for new definitions or ingress resources, and configure the Nginx server accordingly. An Nginx controller is deployed as just another deployment in Kubernetes. So, we start with a deployment definition file named nginx-ingress-controller with one replica and a simple pod definition template. We will label it nginx-ingress, and the image used is nginx-ingress-controller with the right version. This is a special build of Nginx built specifically to be used as an ingress controller in Kubernetes, so it has its own set of requirements. Within the image, the Nginx program is stored at location /nginx-ingress-controller, so you must pass that as the command to start the Nginx controller service.

## Configuring the Ingress Controller
If you have worked with Nginx before, you know that it has a set of configuration options such as the path to store the logs, the keep alive threshold, SSL settings, session timeout, etc. In order to decouple this configuration data from the Nginx controller image, you must create a config map object and pass that in. The config map object need not have any entries at this point. A blank object will do, but creating one makes it easy for you to modify a configuration setting in the future. You will just have to add it into this config map and not have to worry about modifying the Nginx configuration files. You must also pass in two environment variables that carry the pod's name and namespace it is deployed to. The Nginx service requires these to read the configuration data from within the pod. Finally, specify the ports used by the ingress controller, which happens to be 80 and 443. We then need a service to expose the ingress controller to the external world, so we create a service of type NodePort with the nginx-ingress label selector to link the service to the deployment.
## Permissions and Service Account
Ingress controllers have additional intelligence built into them to monitor the Kubernetes cluster for ingress resources, and configure the underlying Nginx server when something has changed. For the ingress controller to do this, it requires a service account with the right set of permissions. For that, we create a service account with the correct rules and rule bindings. To summarize, with a deployment of the nginx-ingress image, a service to expose it, a config map to feed Nginx configuration data, and a service account with the right permissions to access all of these objects, we should be ready with an ingress controller in its simplest form.


## Creating Ingress Resources
An ingress resource is a set of rules and configurations applied on the ingress controller. You can configure rules to simply forward all incoming traffic to a single application, or route traffic to different applications based on the URL. For example, if the user goes to my-online-store.com/wear, then route to one app, or if the user visits the watch URL, then route the user to the video app. Or you could route users based on the domain name itself. For example, if the user visits wear.my-online-store.com, then route the user to the wear app, or else route the user to the video app. The ingress resource is created with a Kubernetes definition file. In this case, Ingress-wear.yaml. As with any other object, we have apiVersion, kind, metadata, and spec. The apiVersion is extensions/v1beta1. Kind is Ingress. We will name it ingress-wear. Under spec, we have backend.


## Backend and Rules in Ingress Resources
The apiVersion for ingress is extension/v1beta1 as of this recording, but this is expected to change with newer releases of Kubernetes. When you are deploying ingress, always remember to refer to the Kubernetes documentation to know exactly the right apiVersion for that release of Kubernetes. The traffic is routed to the application services and not pods directly. The backend section defines where the traffic will be routed to. If it is a single backend, then you do not really have any rules. You can simply specify the service name and port of the backend wear service. Create the ingress resource by running the kubectl create command. View the created ingress by running the kubectl get ingress command. The new ingress is now created and routes all incoming traffic directly to the wear service.

## Using Rules for Advanced Routing
You use rules when you want to route traffic based on different conditions. For example, you create one rule for traffic originating from each domain or host name. That means when users reach your cluster using the domain name, my-online-store.com, you can handle that traffic using rule 1. When users reach your cluster using domain name, wear.my-online-store.com, you can handle that traffic using a separate rule, rule 2. Use rule 3 to handle traffic from watch.my-online-store.com and use the fourth rule to handle everything else. You could get different domain names to reach your cluster by adding multiple DNS entries all pointing to the same ingress controller service on your Kubernetes cluster.

## Path-Based Routing within Rules
Within each rule, you can handle different paths. For example, within rule 1, you can handle the wear path to route that traffic to the clothes application and a watch path to route traffic to the video streaming application, and a third path that routes anything other than the first two to a 404 not found page. Similarly, the second rule handles all traffic from wear.my-online-store.com. You can have path definitions within this rule to route traffic based on different paths. For example, say you have different applications and services within the apparel section for shopping, returns, or support. When a user goes to wear.my-online-store.com, by default, they reach the shopping page. But if they go to exchange or support URL, they reach a different backend service. The same goes for rule 3, where you route traffic to watch.my-online-store.com to the video streaming application, but you can have additional paths in it such as movies or TV. Anything other than the ones listed here will go to the fourth rule that would simply show a 404 not found error page.


## Host and Path Rules Structure
You have rules at the top for each host or domain name, and within each rule, you have different paths to route traffic based on the URL. Let us look at how we configure ingress resources in Kubernetes. We will start where we left off. We start with a similar definition file. This time under spec, we start with a set of rules. Our requirement here is to handle all traffic coming into my-online-store.com and route them based on the URL path. So, we just need a single URL for this, since we are only handling traffic to a single domain name, which is my-online-store.com in this case. Under rules, we have one item, which is an HTTP rule in which we specify different paths. Paths is an array of multiple items, one path for each URL. Then, we move the backend we used in the first example under the first path. The backend specification remains the same, it has a service name and service port. Similarly, we create a similar backend entry to the second URL path for the watch service to route all traffic coming in through the watch URL to the watch service. Create the ingress resource using the kubectl create command. Once created, view additional details about the ingress resource by running the kubectl describe ingress command. You now see two backend URLs under the rules and the backend service they are pointing to, just as we created it.

## Default Backend in Ingress
If you look closely in the output of this command, you see that there is something about a default backend. If a user tries to access a URL that does not match any of these rules, then the user is directed to the service specified as the default backend. In this case, it happens to be a service named default-http-backend. You must remember to deploy such a service. Back in your application, say a user visits the URL, my-online-store.com/listen or eat, and you do not have an audio streaming or a food delivery service, you might want to show them a nice message. You can do this by configuring a default backend service to display this 404 not found error page.

## Host-Based Routing Example
The third type of configuration is using domain names or host names. We start by creating a similar definition file for ingress. Now that we have two domain names, we create two rules, one for each domain. To split traffic by domain name, we use the host field. The host field in each rule matches the specified value with the domain name used in the request URL, and routes traffic to the appropriate backend. In this case, note that we only have a single backend path for each rule, which is fine. All traffic from these domain names will be routed to the appropriate backend irrespective of the URL path used. You can still have multiple path specifications in each of these to handle different URL paths.

## Comparing Path and Host-Based Routing
Splitting traffic by URL had just one rule, and we split the traffic with two paths. To display traffic by host name, we used two rules and one path specification in each rule.