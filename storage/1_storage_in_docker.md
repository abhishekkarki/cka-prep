# Storage in Docker

## Introduction to Docker Storage
We are going to talk about Docker storage drivers and file systems. We will see where and how Docker stores data and how it manages file systems of the containers.

## Docker's Data Storage Structure
When you install Docker on a system, it creates a folder structure at `/var/lib/docker/`. There are multiple folders under it called aufs, containers, image, volumes, etc. This is where Docker stores all its data by default. When we say data, we mean files related to images and containers running on the Docker host. For example:

All files related to containers are stored under the containers folder.

Files related to images are stored under the image folder.

Any volumes created by Docker containers are created under the volumes folder.

For now, let us just understand where Docker stores its files and in what format.

## Docker's Layered Architecture
To understand how Docker stores the files of an image and a container, we need to understand Docker's layered architecture. When Docker builds images, it builds them in a layered architecture. Each line of instruction in the Dockerfile creates a new layer in the Docker image, with just the changes from the previous layer.

For example:

The first layer is a base Ubuntu operating system.

The second instruction creates a second layer which installs all the APT packages.

The third instruction creates a third layer with the Python packages.

The fourth layer copies the source code over.

The fifth layer updates the entry point of the image.

Since each layer only stores the changes from the previous layer, it is reflected in the size as well. If you look at the base Ubuntu image, it is around 120 megabytes in size. The APT packages that are installed are around 300 MB, and then the remaining layers are small.

## Layer Reuse and Efficiency
To understand the advantages of this layered architecture, let us consider a second application. This application has a different Dockerfile but is very similar to our first application, as it uses the same base image as Ubuntu, uses the same Python and Flask dependencies, but uses different source code to create a different application and a different entry point as well.
When we run the Docker build command to build a new image for this application, since the first three layers of both applications are the same, Docker is not going to build the first three layers. Instead, it reuses the same three layers it built for the first application from the cache and only creates the last two layers with the new sources and the new entry point. This way, Docker builds images faster and efficiently saves disk space.

This is also applicable if you update your application code. Whenever you update your application code, such as the app.py in this case, Docker simply reuses all the previous layers from cache and quickly rebuilds the application image by updating the latest source code, thus saving a lot of time during rebuilds and updates.

## Image Layers and Container Layers
Let us rearrange the layers bottom-up for better understanding. At the bottom, we have the base Ubuntu layer, then the packages, then the dependencies, then the source code of the application, and then the entry point. All of these layers are created when we run the docker build command to form the final Docker image. These are the Docker image layers. Once the build is complete, you cannot modify the contents of these layers. They are read-only and can only be modified by initiating a new build.

When you run a container based off this image using the docker run command, Docker creates a container based off these layers and creates a new writable layer on top of the image layers. The writable layer is used to store data created by the container, such as log files written by the applications, any temporary files generated by the container, or any file modified by the user on that container. The life of this layer is only as long as the container is alive. When the container is destroyed, this layer and all the changes stored in it are also destroyed.

## Sharing Image Layers and Copy-on-Write
Remember that the same image layer is shared by all containers created using this image. If you log into the newly created container and create a new file called temp.txt, it will create that file in the container layer, which is read and write. The files in the image layer are read-only, meaning you cannot edit anything in those layers.

For example, since we bake our code into the image, the code is part of the image layer and is read-only after running a container. If you wish to modify the source code to test a change, Docker automatically creates a copy of the file in the read-write layer before you save the modified file. You will then be modifying a different version of the file in the read-write layer. All future modifications will be done on this copy of the file in the read-write layer. This is called the **copy-on-write** mechanism. The image layer being read-only means that the files in these layers will not be modified in the image itself. The image will remain the same until you rebuild the image using the docker build command.

## Persistence and Volumes
When you get rid of the container, all of the data that was stored in the container layer also gets deleted. The change made to app.py and the new temp file created will also get removed. If you wish to persist this data, for example, if you are working with a database and would like to preserve the data created by the container, you can add a persistent volume to the container.
To do this, first create a volume using the docker volume create command.

```bash
#Bash Code Sample
docker volume create data_volume
```
When you run the Docker container using the docker run command, you can mount this volume inside the Docker container's writable layer using the -v option.

```bash
# Bash Code Sample
docker run -v data_volume:/var/lib/mysql mysql
```

This will create a new container and mount the data volume into the `/var/lib/mysql` folder inside the container. All data written by the database is stored on the volume created on the Docker host. Even if the container is destroyed, the data is still active.

If you did not run the docker volume create command to create the volume before the docker run command, Docker will automatically create a volume with the specified name and mount it to the container.

```bash
# Bash Code Sample
docker run -v data_volume2:/var/lib/mysql mysql
```
You should be able to see all these volumes if you list the contents of the /var/lib/docker/volumes folder. This is called volume mounting, as we are mounting a volume created by Docker under the `/var/lib/docker/volumes` folder.

## Bind Mounts
If you have data already at another location, for example, some external storage on the Docker host at /data, and you would like to store database data on that volume and not in the default /var/lib/docker/volumes folder, you can run a container using the following command:

```bash
# Bash Code Sample
docker run -v /data/mysql:/var/lib/mysql mysql
```

This will create a container and mount the folder to the container. This is called bind mounting. There are two types of mounts:

Volume mounting: mounts a volume from the volumes directory.

Bind mount: mounts a directory from any location on the Docker host.

## The --mount Option
Using the -v option is the old style. The new way is to use the --mount option. The --mount is the preferred way as it is more verbose, so you have to specify each parameter in a key equals value format. For example, the previous command can be written with the --mount option as follows:
```bash
# Bash Code Sample
docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql
```

The type in this case is bind, the source is the location on the host, and the target is the location in the container.

## Docker Storage Drivers
Who is responsible for maintaining the layered architecture, creating a writable layer, and moving files across layers to enable copy-on-write? It is the storage drivers. Docker uses storage drivers to enable the layered architecture. Some of the common storage drivers are AUFS, BTRFS, ZFS, Device Mapper, Overlay, and Overlay2. The selection of the storage driver depends on the underlying operating system being used. For example, with Ubuntu, the default storage driver is AUFS, whereas this storage driver is not available on other operating systems like Fedora or CentOS. In that case, Device Mapper may be a better option. Docker will choose the best storage driver available automatically based on the operating system. The different storage drivers also provide different performance and stability characteristics, so you may want to choose one that fits the needs of your application and your organization.

If you would like to read more on any of these storage drivers, please refer to the links in the attached documentation. For now, that is all from the Docker architecture concepts.
